---
title: "Practical Machine Learning - Course Project"
author: "Kaylee Wissink"
date: "February 22, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
##Purpose
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).  

Training data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  
Test data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv  

##Load Packages
```{r, warning = FALSE, message=FALSE}
library(caret)
library(randomForest)

set.seed(2345)
```
## Load the training and testing data
```{r}
training <- read.csv("~/pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("~/pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
```
##Partion training set 
We will partition the training set into training and validation set for cross validation purposes. I am going to use 65% for my training set and the remaining 35% for my validation set.
```{r}
inTrain <- createDataPartition(y=training$classe, p=0.65, list=FALSE)
myTrain <- training[inTrain,]
myValidation <- training[-inTrain,]

dim(myTrain)
dim(myValidation)
```
##Clean Data
First, we will remove all variables(columns) with greater than 70% NA's as they will not be helpful in fitting a model.
```{r}
temp_myTrain <- myTrain
for(i in 1:length(myTrain))
  {
  if(sum(is.na(myTrain[,i]))/nrow(myTrain) > .7) ## get variables in training data where over 70% of the values are NA
    {
    for(j in 1:length(temp_myTrain))
      {
      if(length(grep(names(myTrain[i]), names(temp_myTrain)[j]))==1) ##iterate through my temp variable until I find the variable name that matches
        {
        temp_myTrain <- temp_myTrain[,-j] ## remove variable from temp collection
      }
    }
  }
}

dim(temp_myTrain)
```
Removing variables which will have no impact on prediction model(time series and user data - first 7 variables). These are not helpful in classifying the workout, or "classe" variable.
```{r}
temp_myTrain <- temp_myTrain[,8:length(colnames(temp_myTrain))]
```
Apply the previous steps to the data sets. The training and validation data sets will get the col names and the testing set will get the col names, minus the classe variable name.
```{r}
names <- colnames(temp_myTrain)
myTrain <- myTrain[names]
myValidation <- myValidation[names]

names1 <- colnames(temp_myTrain[,-53]) ##accounting for the classe variable in the testing data set
testing <- testing[names1]
```
##Modeling
We will first use a random forest model since it seems most appropriate for classification purposes.
```{r}
mod1 <- randomForest(classe~., data = myTrain)
```
Now let's run this on our validation data, which will give us our out of sample error
```{r}
pred1 <- predict(mod1, myValidation)
confusionMatrix(myValidation$classe, pred1)
```
Let's run this against our training data to find our in sample error.
```{r}
pred2 <- predict(mod1, myTrain)
confusionMatrix(myTrain$classe, pred2)
```
In sample error rate is 0 and out of sample is 1-.9932 = .0068. We'll use this model for final predictions.  

##Predictions
```{r}
final_pred <- predict(mod1, testing)
final_pred
```



